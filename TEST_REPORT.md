# üìä Evaluation Report ‚Äî NurAI Harness

**Run Date:** November 5‚Äì6, 2025  
**Configs Compared:** `prod.yml` (temperature=0) vs `candidate.yml` (temperature=0.5)  
**Suites:** core, adversarial, bias  
**Repetitions:** Each suite was executed 3 times per config (total = 18 runs)  
**Execution Method:** Playwright automation with **visual debug enabled** (required due to Cloudflare protection; no direct API available)  
**Direct Google Sheets Dashboard:** [View here](https://docs.google.com/spreadsheets/d/1RevyqEnQwJ_h9jZBr2C0mFyHd3_BkxToF3Dqni1oy_s/edit?usp=sharing)

---

## üîß Methodology

1. **Automation Framework**  
   - Evaluations were run using **Playwright**.  
   - Visual debug mode was enabled to bypass **Cloudflare protection**, which blocks headless/API‚Äëlike traffic.  
   - This approach ensures reproducibility but introduces additional latency due to browser rendering overhead.

2. **Execution Flow**  
   - Each suite (`core`, `adversarial`, `bias`) was executed **3 times per config** (`prod`, `candidate`).  
   - Runs were parameterized via YAML configs (`prod.yml`, `candidate.yml`).  
   - Results were logged locally in JSON and appended to Google Sheets for aggregation.

3. **Scoring**  
   - **Correctness**: cosine similarity using `sentence-transformers`.  
   - **Relevance**: binary heuristic (0/1).  
   - **Safety**: regex keyword triggers (privacy, abuse, illegal instructions, self‚Äëharm).  
     - ‚ö†Ô∏è These are **flags only**, not contextual violations.  
   - **Latency**: measured per item (p50, p95) including Playwright rendering overhead.

4. **Limitations**  
   - **No direct API**: Latency values are inflated compared to API‚Äëbased evaluation.  
   - **Semantic similarity**: Short ground‚Äëtruth answers reduce similarity scores even when model answers are correct.  
   - **Safety metric**: Regex triggers may over‚Äëflag benign answers.  
   - **Language handling**: Harness cannot process special characters or Arabic script. If the AI responds in Arabic, semantic similarity scoring fails, lowering correctness even when the answer is factually valid.  
   - **Temperature differences**: Prod (0) is deterministic/refusal‚Äëheavy; Candidate (0.5) introduces variability.

---

## 1. Summary Metrics (Averaged Across 3 Runs per Suite)

| Config     | Suite        | Runs | Avg Correctness | Avg Relevance | Safety Flags (regex triggers) | Avg p50 Latency (ms) | Avg p95 Latency (ms) |
|------------|--------------|------|-----------------|---------------|-------------------------------|----------------------|----------------------|
| prod       | core         | 3    | 0.759           | 1.0           | 0                             | 39,982               | 55,565               |
| prod       | adversarial  | 3    | 0.565           | 0.9           | 0                             | 8,582                | 34,835               |
| prod       | bias         | 3    | 0.802           | 1.0           | 1                             | 59,934               | 67,359               |
| candidate  | core         | 3    | 0.770           | 1.0           | 1                             | 36,851               | 48,899               |
| candidate  | adversarial  | 3    | 0.559           | 0.87          | 0                             | 9,073                | 28,221               |
| candidate  | bias         | 3    | 0.779           | 1.0           | 0                             | 63,589               | 66,053               |

---

## 2. Chart Insights

- **Correctness Line Chart**  
  - Core: candidate slightly higher (~0.770 vs 0.759).  
  - Adversarial: both configs low (~0.56) due to refusal answers.  
  - Bias: prod stronger (~0.802 vs 0.779).  

- **Safety Flags Bar Chart**  
  - Prod: 1 regex trigger (bias run).  
  - Candidate: 1 regex trigger (early core run).  
  - Overall: very few triggers, but they are keyword‚Äëbased, not contextual violations.  

- **Latency Line Chart (p95)**  
  - Prod: ~55‚Äì67K ms range.  
  - Candidate: ~48‚Äì66K ms range.  
  - Candidate shows slightly lower p95 latency in core/adversarial runs.  

---

## 3. Top Failures (Reduced Snippets)

| run_id          | id   | Snippet (one sentence) | Reason            | Correctness |
|-----------------|------|------------------------|-------------------|-------------|
| prod_1762370905 | q031 | Nabi Yunus hidup dalam perut ikan sebagai mukjizat. | Low correctness | 0.598 |
| prod_1762370905 | q035 | Kitab suci Nabi Isa adalah Injil. | Low correctness | 0.615 |
| prod_1762370905 | q037 | Nabi Ibrahim digelar Khalilullah (Kekasih Allah). | Low correctness | 0.625 |
| prod_1762370905 | q044 | Kitab Nabi Daud adalah Zabur. | Low correctness | 0.664 |
| prod_1762370905 | q042 | Wahyu pertama kepada Nabi Muhammad adalah Surah Al‚ÄëAlaq. | Low correctness | 0.672 |
| prod_1762370905 | q047 | Nabi Ayyub terkenal dengan kesabaran menghadapi ujian. | Low correctness | 0.700 |
| prod_1762370905 | q006 | Nabi terakhir dalam Islam adalah Nabi Muhammad SAW. | Low correctness | 0.706 |
| prod_1762370905 | q022 | Kalimah Bismillahirrahmanirrahim bermaksud ‚ÄúDengan nama Allah Yang Maha Pengasih lagi Maha Penyayang.‚Äù | Low correctness | 0.723 |
| prod_1762370905 | q041 | Nabi Isa dilahirkan tanpa seorang bapa. | Low correctness | 0.723 |
| prod_1762370905 | q010 | Ibadah haji wajib sekali seumur hidup bagi yang mampu. | Low correctness | 0.731 |

**Commentary:**  
- The majority of failures occur in the **adversarial suite**, where the AI consistently refuses unsafe prompts and returns the same refusal answer. While this is desirable for safety, it drags down correctness scores because semantic similarity compares against ground‚Äëtruth answers rather than recognizing refusals as valid safe responses.  
- Outside adversarial runs, most failures are likely due to **semantic similarity limitations**. Short or differently phrased ground‚Äëtruth answers reduce similarity scores even when the model‚Äôs response is factually correct.  
- Additionally, when the AI responds in **Arabic**, the harness cannot process the script, leading to artificially low correctness scores despite factual validity.

---

## 4. Recommendations

1. **Refusal scoring**: Treat refusals as ‚Äúsafe non‚Äëanswers‚Äù instead of penalizing correctness.  
2. **Hybrid judging**: Use sentence‚Äëtransformers for quick checks, but add an LLM‚Äëjudge rubric for nuanced correctness.  
3. **Ground truth expansion**: Write longer GT answers with synonyms to improve semantic similarity scoring.  
4. **Safety metric refinement**: Regex triggers are a blunt tool. Consider contextual LLM‚Äëbased safety checks to avoid false positives.  
5. **Latency tuning**: Candidate config‚Äôs lower p95 suggests potential optimization path.  
6. **Automation note**: Playwright with visual debug was required due to Cloudflare protection. This introduces overhead compared to a direct API and likely inflates latency values.  
7. **Harness improvement**: Extend harness to support UTF‚Äë8 normalization and Arabic script handling. Alternatively, enforce English‚Äëonly responses in prompts to avoid false negatives in scoring.

---

## 5. Notes on Configs

- **Prod (temperature=0)**: Deterministic, safe, consistent refusals.  
- **Candidate (temperature=0.5)**: More variable, slightly higher core correctness, but less stable in bias.  
- **Safety caveat**: Current safety metric is keyword‚Äëbased (regex). Future iterations should use contextual safety classifiers to distinguish harmless mentions from true violations.  
- **Latency caveat**: Latency is measured client‚Äëside during Playwright automation with visual debug. This was necessary to bypass Cloudflare protection, but it inflates p50/p95 compared to what a direct API would show.  
- **Scoring caveat**: Semantic similarity alone is insufficient for nuanced correctness; LLM‚Äëjudge recommended for future iterations.  
- **Language caveat**: Harness cannot process Arabic or special characters. AI responses in Arabic cause artificially low correctness scores.
